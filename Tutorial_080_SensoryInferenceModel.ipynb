{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic model of perceptual inference\n",
    "* Basic implementation of Jazayeri and Movshon, 2006 Nature Neuroscience\n",
    "\n",
    "* The model  addresses the issue of how to combine the output of multiple sensory neurons in order to infer the most likely state of a stimulus that is driving the observed pattern of responses across the entire population of cells. This is a classic problem in sensory neuroscience (or really any area where you're combining noisy/ambiguous signals to make an optimal inference). The reason the problem is so interesting is that the output of a single neuron is almost useless for performing inference, even if the neuron has a highly stable output (e.g. an orientation tuning function) that can be robustly measured. This ambiguity arises for a few reasons. \n",
    "\n",
    "* First, there is variability (the unpredictable kind, i.e. 'noise') in the output of neurons - so a response of 50Hz might be observed to stimulus 1 on trial 1, but a response of 40Hz might be observed on trial 2, etc. Second, the tuning function of most sensory neurons is non-monotonic (e.g. Gaussian-ish), so that almost all repsonse states are consistent with at least two possible stimuli (even in the complete absence of noise). Thus, a single measurement from a single neuron cannot be used to reliably transmit much information at all about the stimulus that was most likely to have caused a response. \n",
    "\n",
    "* Instead, inference about sensory stimuli is thought to be based on the output of many sensory neurons that are tuned to different points across a given feature space. For example, if you have just two neurons, and they are tuned to different points in feature space, it immediately becomes much easier to discriminate which feature was presented based on the output PATTERN across the two neurons. The more neurons you add (generally speaking) the more accurate your inference will be in the presence of noise because each neuron will contribute a little bit to disambiguating the input feature. \n",
    "\n",
    "* Note also that this approach generates a full estimate of the likelihood function, not just a point estimate of the most likely stimulus. This is a key feature of the approach, because different tasks require the implementation of different decision rules. For example, you might choose the most likely stimulus in the context of a discrimination, but you would want to compare the likelihood to some adjustable criterion. Furthermore, if you want to integrate multiple sources of information, such as multisensory integration or combining sensory evidence with a prior, then you need to deal in likelihood functions, not point estimates. This is a really critical advantage of this model over other approaches that optimally (i.e. use all available information) to determine just a point estimate of the most likely stimulus. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import pi as pi\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# also define the default font we'll use for figures. \n",
    "fig_font = {'fontname':'Arial', 'size':'20'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First some stimulus parameters - just going to simulate motion is different directions (i.e. a random dot stimulus ala Newsome). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the direction of the stimulus that we'll try to decode\n",
    "s1 = pi             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then set up some params that define the stimulus space (i.e. not the stim that we're presenting per se, but the space of all possible stimuli that we might evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of possible directions\n",
    "nDirs = 72   \n",
    "\n",
    "# resolution sampling across direction space (radians)\n",
    "dirStepSize = (2*pi)/nDirs             \n",
    "\n",
    "# direction of each stimulus\n",
    "sDir = np.linspace(0, (2*pi)-dirStepSize, nDirs)  \n",
    "\n",
    "# xaxis of possible directions for plotting (Degrees)\n",
    "xAxis = np.arange(0,360,360/nDirs)                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then some params to define our population of neurons\n",
    "* Define the number of cells - we can play with this later\n",
    "* Define the step size between the peaks of the tuning functions and then compute the actual peak of each tuning curve.\n",
    "* Then define the 'concentration parameter' of each tuning function (the width of the circular Gaussian TF that we'll use)\n",
    "* Want noise?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of neurons\n",
    "N = 72          \n",
    "\n",
    "# resolution sampling across direction space (radians)\n",
    "stimStepSize = (2*pi)/nDirs  \n",
    "\n",
    "# set up the direction preference of each neuron\n",
    "nDir = np.linspace(0, (2*pi)-2*pi/N, N) \n",
    "\n",
    "# then some other factors - tuning function width and any noise\n",
    "# we want to add to the responses\n",
    "k = 7            # concentration param for tuning curve \n",
    "noise = .05        # noise (IID) of neural responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then build the actual tuning function for each neuron\n",
    "* Use a circular Gaussian (Von Mises function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over all possible sigs\n",
    "fx = np.zeros((N,nDirs))\n",
    "\n",
    "# loop over neurons...each of which is tuned to nDir[i]\n",
    "for i in np.arange(N): \n",
    "    fx[i,:] = np.exp(k*(np.cos(sDir-nDir[i])-1))\n",
    "                     \n",
    "# set up a plotting axis in degrees (from radians)\n",
    "xaxis = sDir*(180/pi)\n",
    "plt.plot(xaxis, fx[0,])\n",
    "plt.plot(xaxis, fx[36,])\n",
    "plt.xlabel('Motion Direction', **fig_font)\n",
    "plt.ylabel('Response', **fig_font)\n",
    "plt.show()\n",
    "\n",
    "# look at the full profile...\n",
    "plt.imshow(fx, cmap='inferno', extent=[0, 360, 0, N])  # 'viridis', 'plasma', 'inferno', 'magma', etc\n",
    "plt.xlabel('Motion Direction', **fig_font)\n",
    "plt.ylabel('Neuron Number', **fig_font)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figures 1-2 from J&M 2006 (and quick demo of LaTeX)\n",
    "* Refer to equation 4 from the paper:\n",
    "\n",
    "\n",
    "$$\\log L(\\theta) = \\sum_{i=1}^{N}n_{i}{\\cos(\\theta - \\theta_{i})}$$\n",
    "\n",
    "\n",
    "* This is a rewriting of equ 3, where you had:\n",
    "\n",
    "$$\\log L(\\theta) = \\sum_{i=1}^{N}n_{i}{\\log f_{i}(\\theta)}$$\n",
    " \n",
    "* So if: \n",
    "\n",
    "$$f(\\theta) = e^{x}$$\n",
    "\n",
    "* then: \n",
    "\n",
    "$$\\log(e^{x}) = x$$\n",
    "    \n",
    "* which gives equation 4:\n",
    "\n",
    "$$\\log L(\\theta) = \\sum_{i=1}^{N}n_{i}{\\cos(\\theta - \\theta_{i})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Given our bank of sensory neurons with tuning functions, generate a likelihood function to eval the most likely stim given pattern of observed responses\n",
    "* First, we'll get the response vector to the stimulus of interest this R vector will correspond to the n1...ni vector in the paper\n",
    "* Bonus - what is wrong with this version of the code??? \n",
    "    * re-run this cell several times with non-zero noise...what happens?\n",
    "    * what is the problem and how to we fix it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sInd = np.argmin(np.abs(s1-sDir))\n",
    "\n",
    "# pull a column out of the matrix that defines our TFs. \n",
    "R = fx[:,sInd]\n",
    "\n",
    "# add noise if desired\n",
    "print('Noise: ', noise)\n",
    "if noise:\n",
    "    R += (np.random.randn(len(R)) * noise)   \n",
    "\n",
    "# plot\n",
    "plt.plot(R, 'k-', linewidth=2)\n",
    "plt.xlabel('Neuron Number', **fig_font)\n",
    "plt.ylabel('Response', **fig_font)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's wrong with the response vector that we have above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set all negative responses to zero...\n",
    "R[R<0]=0\n",
    "\n",
    "# plot\n",
    "plt.plot(R, 'k-', linewidth=2)\n",
    "plt.xlabel('Neuron Number', **fig_font)\n",
    "plt.ylabel('Response', **fig_font)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Then generate the weights to apply to R. Notice that this is exactly what the cos(theta-theta_pref_i) does...and its related to the magnitude of the response. By 'weight' here I just mean that the neurons that are most tuned to the stim will have the highest SNR response and should thus be 'trusted' the most in computing the log likelihood function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the part of the equation from above that computes the weight\n",
    "w = np.cos(s1 - nDir) \n",
    "\n",
    "# plot\n",
    "plt.plot(w, 'k-', linewidth=2)\n",
    "plt.xlabel('Neuron Number', **fig_font)\n",
    "plt.ylabel('Weight towards LL', **fig_font)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then compute the full LL for each possible stimulus in our space by looping over neuron, getting its response, and weighting it by its tuning funtion's offset to the stimulus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initialize an array of zeros...\n",
    "logl = np.zeros(nDirs)\n",
    "\n",
    "for s in np.arange(nDirs):\n",
    "    logl[s] = k * np.sum(R*np.cos(sDir[s]-nDir))\n",
    "                \n",
    "# plot\n",
    "plt.plot(logl, 'k-', linewidth=2)\n",
    "plt.xlabel('Motion direction', **fig_font)\n",
    "plt.ylabel('logL', **fig_font)\n",
    "plt.show()\n",
    "\n",
    "#MAP estimate of stim direction!\n",
    "print('Estimated motion direction:', sDir[np.argmax(logl)]*180/pi, ' Degrees') \n",
    "print('Actual motion direction:', s1*180/pi, ' Degrees') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now make Figure 4 from J&M 2006 (sort of) - how to do discrimination between two stims?\n",
    "* first define 2 stims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = 0        # noise (IID) of neural responses, set to 0 to make this easier to understand...\n",
    "s1 = pi\n",
    "s2 = pi*2 \n",
    "s2 = pi+pi/16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now regenerate responses..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over all possible sigs\n",
    "fx = np.zeros((N,nDirs))\n",
    "\n",
    "# loop over neurons...each of which is tuned to nDir[i]\n",
    "for i in np.arange(N): \n",
    "    fx[i,:] = np.exp(k*(np.cos(sDir-nDir[i])-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We want to determine which stimulus is most likely given an observed pattern of responses. \n",
    "\n",
    "* One common scheme is the neuron/anti-neuron case developed by Newsome in which the likelihood of a given stimulus is a function of activity in neurons tuned to the stimulus minus the activity of neurons tuned to the opposite stimulus (the evil \"anti-neurons\"). This works fine, but it ignores the contribution of all neurons that aren't tuned to the taregt stimulus (or to the anti-target-stimulus)...i.e. its not an optimal model. \n",
    "\n",
    "* An optimal approach, or one that uses all available information, is specified by the present model. \n",
    "\n",
    "* What we want is to compute the ratio of the likelihood of each stimulus, so:\n",
    "\n",
    "$$LR = L(\\theta_{1}) / L(\\theta_{2})$$\n",
    "\n",
    "\n",
    "* Going to log space we would have:\n",
    "\n",
    "$$logLR = \\log L(\\theta_{1}) - \\log L(\\theta_{2})$$\n",
    "\n",
    "* Which is: \n",
    "\n",
    "$$logLR = k * \\sum_{i=1}^{N}n_{i}[cos(\\theta_{1} - \\theta_{i})-cos(\\theta_{2} - \\theta_{i})]$$\n",
    "\n",
    "* Or\n",
    "\n",
    "$$LogLRatio = k * \\sum_{i=1}^{N}(R_{i}[cos(\\theta_{1} - \\theta_{i})-cos(\\theta_{2} - \\theta_{i})]$$\n",
    "\n",
    "* And \n",
    "\n",
    "$$cos(\\theta_{1} - \\theta_{i})$$\n",
    "\n",
    "is the expected response of each neuron to s1, or the weights that we want to apply to the observed response profile to compute the likelihood of s1...\n",
    "\n",
    "* And then we do the same for s2. \n",
    "\n",
    "* So the weighting function when computing the ratio of the likelihoods is the difference between the weighting functions associated with each individual stimulus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat our steps to figure out what the response profile is across neurons for the \n",
    "# stimulus that was actually presented (lets say it was s1)\n",
    "sInd = np.argmin(np.abs(s1-sDir))\n",
    "\n",
    "# pull a column out of the matrix that defines our TFs. \n",
    "R = fx[:,sInd].copy()\n",
    "\n",
    "if noise:\n",
    "    R += np.random.randn(len(R))*noise\n",
    "\n",
    "# fix any negative responses...\n",
    "R[R<0] = 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the weights for s1 and s2 - this is determined by the expected response of a neuron to each stimulus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = np.cos(s1-nDir)\n",
    "w2 = np.cos(s2-nDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contribution of neurons to logL(s1) - this is the actual response of each neuron multiplied by the weight of each neuron (which is the TF, or the expected response of the neuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logL1=R*w1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then contribution of each neuron to logL(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logL2=R*w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute contribution of each neuron to LLR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logLR_cont = k * (R*(w1-w2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the actual log likelihood ratio which is the sum of LLR for each neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logLR = k * np.sum(R*(w1-w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data\n",
    "nAxis = nDir*180/pi;\n",
    "nInd = np.argmin(np.abs(s1*(180/pi)-nAxis)) # find the stimulus index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot...\n",
    "plt.plot(nAxis, R, 'o-', linewidth=2)\n",
    "plt.xlabel('Motion Direction', **fig_font)\n",
    "plt.ylabel('Response', **fig_font)\n",
    "plt.title('Observed response across neurons', **fig_font)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(nAxis, w1, '-', linewidth=2)\n",
    "plt.plot(nAxis, w2, ':', linewidth=2)\n",
    "plt.xlabel('Motion Direction', **fig_font)\n",
    "plt.ylabel('Weights', **fig_font)\n",
    "plt.title('Weights for s1 and s2', **fig_font)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(nAxis, logL1, linewidth=2)\n",
    "plt.plot(nAxis, logL2, ':', linewidth=2)  \n",
    "plt.xlabel('Motion Direction', **fig_font)\n",
    "plt.ylabel('Contribution of each Neuron to LL', **fig_font)\n",
    "plt.title('Contribution of each Neuron to LL', **fig_font)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(nAxis, logL1-logL2, linewidth=2)\n",
    "plt.xlabel('Motion Direction', **fig_font)\n",
    "plt.ylabel('Contribution of each Neuron to LLR', **fig_font)\n",
    "plt.title('Contribution of each Neuron to LLR', **fig_font)\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
