{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy, conditional entropy, and mutual information\n",
    "\n",
    "Overview:\n",
    "* In this tutorial, we will learn how to describe the information that is shared between two variables (mutual information). In other words, how much uncertainty reduction is there to be had about variable 1 by measuring variable 2? \n",
    "\n",
    "* These concepts were initially developed in communication theory to describe the efficacy of transmitting signals over a noisy medium (like a noisy telephone line). For example, suppose that we want to know how good a communication channel is, or its effeciency in reliably relaying a message from point A (a 'sender') to point B (a 'reciever').\n",
    "\n",
    "* Basically this is just like asking, \"we know how good the signal is at A, and we recieved the message at B - how much information about A is still in the received signal B?\". \n",
    "\n",
    "* So that is the general gist of it, but right away you can see the potential applicability of this metric in many fields of neuroscience, psychology, engeneering, etc. In neuroscience, we're bascially dealing with a series of communication channels that are corrupted by noise (i.e. synapses). It is therefore reasonable to ask: how much information from neuron A effectively propogates to neuron B? (or conversely, how much  information is lost?).\n",
    "\n",
    "* However, this logic works for any combination of variables: two continuous variables, two discrete variables, one continuous and one discrete, etc. As a result, we can ask questions about any two variables really: how much information about median home  price is reflected in stock market fluctuations? etc.\n",
    "\n",
    "* A few notes before we get started. First, we're going to be talking a lot about 'uncertainty' and 'uncertainty reduction'. While this is basically complementary to talking about certainty and an increase in certainty, we'll deal with the former terminology as it is embedded in some of the concepts that we'll discuss. \n",
    "\n",
    "* Second, we'll be dealing with variability in data, and how we can either attribute that variability in the data to 'noise' or to 'signals'. I.e. is the variability in one variable random wrt another variable? or does the variability in one variable systematically change with the variability in another? \n",
    "\n",
    "* In addition, MI has has a very intuitive interpretation in terms of the amount of information that is shared between two variables, and we'll get to that in a few minutes. \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Finally, a lot of people think at this point, \"why not just correlate the variables using a normal r-value?\". There are a few answers to this, but the simplest is this: correlation assumes a linear relationship (or, in more complex forms, a known relationship or you have to assume a relationship) between variables. Mutual information does not, and can generally capture any form of linear or non-linear relationship between two variables. This makes it a very powerful and general purpose metric.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[jackknife correction](https://www.pnas.org/content/115/40/9956)\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "From the above linked paper from Zeng, Xia, and Tong (their Abstract): Quantifying the dependence between two random variables is a fundamental issue in data analysis, and thus many measures have been proposed. Recent studies have focused on the renowned mutual information (MI) [Reshef DN, et al. (2011) Science 334:1518–1524]. However, “Unfortunately, reliably estimating mutual information from finite continuous data remains a significant and unresolved problem” [Kinney JB, Atwal GS (2014) Proc Natl Acad Sci USA 111:3354–3359]. In this paper, we examine the kernel estimation of MI and show that the bandwidths involved should be equalized. We consider a jackknife version of the kernel estimate with equalized bandwidth and allow the bandwidth to vary over an interval. We estimate the MI by the largest value among these kernel estimates and establish the associated theoretical underpinnings.\n",
    "</div>\n",
    "\n",
    "* This is a very important concept to deal with - MI, esp for continuous variables, is highly unstable and requires correction procedures to counter the bias that is inherent in estimating MI for small data sets. \n",
    "* So while at the start of the tutorial we'll use discrete arrays of numbers that have few unique entries to demonstrate the basic concepts (like binary weighted coin flips, for example), things will get a bit crazier when we move on to continuous arrays of values. \n",
    "* Note also that the proposed jacknife correction from this PNAS paper is just one approach...I'm implementing it because in the few cases I've tried, it seems to be pretty numerically stable. Hwoever, there are other slightly more straightforward approaches that build on the ideas that we discussed in the \"randomization\" tutorial a few weeks back (and indeed,the jacknife approach is logically related as well).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.special import gamma,psi\n",
    "from scipy import ndimage\n",
    "from scipy.linalg import det\n",
    "from numpy import pi\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%% PART II: Entropy as a measure of variability\n",
    "% Shannon Entropy is related to the variability of\n",
    "% data, but is more specifically defined as the average uncertainty in a\n",
    "% variable. Consider coin tosses - lets say we have a balanced coin, and we\n",
    "% flip it once. We can represent the outcome of a single toss as a 0 or a 1\n",
    "% (a head or a tail), or an entropy of 1 bit. If we flipped the coin twice, \n",
    "% then we would have an entropy of 2 bits (00, 01, 10, or 11). When the\n",
    "% coin is balanced and each outcome is equally likely, then entropy is\n",
    "% highest and flipping the coin will convey 1 bit of information. \n",
    "% To see why entropy is maximized in this situation, consider a biased coin \n",
    "% that comes up head 60% of the time. We could predict the outcome of the coin flip more often than\n",
    "% chance simply by going with our prior of 'head'. Thus, the entropy of the\n",
    "% biased coin flip is less than the entropy of the unbiased coin flip, as\n",
    "% the reduction in our uncertainty about the outcome is lower with the\n",
    "% biased coin than with the balanced coin. Put another way, we learn less\n",
    "% after flipping the biased coin than we do when we flip the unbiased\n",
    "% coin. \n",
    "% These examples bring up an important point: entropy as a measure of the\n",
    "% unpredictability of information (or unpredictability of an outcome) is\n",
    "% maximized when all possible outcomes are equally likely. In this case,\n",
    "% making a measurement will carry a lot of information, as you have no\n",
    "% priors upon which to base a guess in advance. In contrast, when only a\n",
    "% subset of all possible outcomes are actually possible, then entropy\n",
    "% is lower as the information gain from making a measurement is not much\n",
    "% beyond your priors (what you already knew).\n",
    "\n",
    "% to figure out the entropy of a measurement in terms of bits (the most common\n",
    "% metric), we can use the log2 function,\n",
    "% referred to as the binary logarithm and the inverse function of 2^N. The\n",
    "% log2(n) is the power to which the number 2 must be raised to obtain the\n",
    "% value n. Lets go back to our coin flip example with a fair coin. Suppose\n",
    "% you flipped the coin once - the entropy would be:\n",
    "\n",
    "log2(2) % log2(n) or 2^x = n? ...x = 1\n",
    "\n",
    "% because there are two possible outcomes. \n",
    "\n",
    "% if you flipped the coin twice you'd have 4 possible outcomes (00,10,01,11)\n",
    "%, or three times you'd have 8 possible outcomes:\n",
    "\n",
    "log2(4)  % or 2^x = n, x = 2\n",
    "log2(8)  % or 2^x = n, x = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% to see the general shape of the function, plot out log2(x:y) and that will tell you the\n",
    "% relationship between the number of possible outcomes and the entropy in\n",
    "% bits\n",
    "clear \n",
    "close all\n",
    "figure(1)\n",
    "plot(1:15, log2(1:15), 'LineWidth', 2)\n",
    "set(gca, 'YLim', [-1,5])\n",
    "ylabel('Entropy (2^N = ?)')\n",
    "xlabel('# of possible outcomes')\n",
    "set(gca, 'FontSize', 24)\n",
    "% note that log2(0) == -inf, and that log2(1)==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% going back to the above notion that entropy is maximial\n",
    "% when the outcome is completely uncertain (e.g. a fair coin), then we can\n",
    "% start to develop an intuition about expressing entropy in terms of the\n",
    "% probability that some event will occur, denoted P(X). Lets take a more\n",
    "% complex case - lets say that we have a slot machine that has two wheels on it\n",
    "% and the first wheel can take one of N states and the second can take one of M states \n",
    "% when we pull the handle. Considering just the first wheel, the possible outcomes are\n",
    "% {x1....xn}, and if each outcome is equally likely, then p(xi)= 1/n. \n",
    "% So, for example, if n = 16, then the total entropy of wheel 1 can be represented \n",
    "% by 4 bits of information(four binary numbers). \n",
    "\n",
    "log2(16)\n",
    "\n",
    "% now what about the second wheel? It can take on M states. So inutively\n",
    "% the total number of possible outcomes for both wheels is N*M. The\n",
    "% uncertainty of the outcome in this case is then:\n",
    "n = 16;\n",
    "m = 16;\n",
    "log2(n*m)\n",
    "\n",
    "% which recall is equal to \n",
    "log2(n) + log2(m) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "% (which gives us a nice way to account for the\n",
    "% probability of outcomes across multiple variables via summation! why is this \n",
    "% computationally convienent, esp on digital computers?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% so in this case we need 8 bits of information to specify all possible\n",
    "% outcomes. Now lets consider each possible outcome in isolation. what is\n",
    "% the uncertainty associated with each event? its the probability of that\n",
    "% event occuring, which is p(xi) = 1/n. So in terms of bits, we have :\n",
    "n = 16;\n",
    "-log2(1/n) % why negative when dealing with probabilities? log2(1)-log2(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%% AVERAGE UNCERTAINTY what we'd like though is a way to assess the average uncertainty of a\n",
    "% particular outcome across all possible outcomes. How would you do that?\n",
    "% Intuitively, you'd take the uncertainty of each outcome (-log2(p(xi))\n",
    "% and weight it by the probabiltiy that the event will actually \n",
    "% occur, like this (where\n",
    "% entropy is denoted, by convention, as H):\n",
    "\n",
    "%H = -sum_over_all_i( p(xi) * log2(p(xi)) )\n",
    "n = 16;\n",
    "H = 0;\n",
    "for i=1:n\n",
    "    H = H + -( (1/n) * log2(1/n) );\n",
    "end\n",
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% what does H, the average uncertaintly, equal when all events are equally likely?\n",
    "% it equals the uncertaintly of each event, which it should. And note that\n",
    "% this framework would let you compute the average uncertainty (entropy)\n",
    "% across both slot machine wheels just as easily by looping over all n an m:\n",
    "n = 16;\n",
    "m = 16;\n",
    "H = 0;\n",
    "for i=1:n\n",
    "    H = H + -( (1/n) * log2(1/n) );\n",
    "end\n",
    "for i=1:m\n",
    "    H = H + -( (1/m) * log2(1/m) );\n",
    "end\n",
    "H  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% what about when all events are not equally likely, which is the more\n",
    "% interesting case to consider.\n",
    "n = 16;\n",
    "px = rand(1,n);\n",
    "px = px./sum(px); % convert the random numbers to probabilities\n",
    "plot(px)\n",
    "H = 0;\n",
    "for i=1:n\n",
    "    H = H + -( px(i) * log2(px(i)) );\n",
    "end\n",
    "H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "% note that the entropy is lower because we've moved away from the point where\n",
    "% everything is maximally unpredictable (i.e. everything is equally\n",
    "% likely).\n",
    "% This demonstrates a principle that relates probability\n",
    "% distributions to entropy: uniform probability distributions have maximum\n",
    "% entropy, and non-uniform distributions will generally have less entropy\n",
    "\n",
    "% SUMMARY - entropy is a measure of uncertainty, and as uncertainty goes up\n",
    "% (maximized when all outcomes are equally likely) then the information\n",
    "% gained by making a measurement goes up. In other words: if you know the\n",
    "% oucome in advance (e.g. a coin with two heads) then p(tails) = 0 and there is no\n",
    "% uncertainty, entropy is 0, and no reduction in uncertainty will be gained by flipping\n",
    "% the coin. If you have a fair coin, then p(head)==p(tail) and entropy will\n",
    "% be maximum and you will greatly reduce uncertainty by making the\n",
    "% measurement (in this case, you will fully disambiguate the outcome,\n",
    "% gaining 1 bit of information where the total uncertainty is 1 bit - so\n",
    "% you get the maximum amount of information). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2;\n",
    "ph = .001:.001:.999;  % vary the probability of heads\n",
    "H = zeros(1,numel(ph));\n",
    "for j=1:numel(ph)\n",
    "    px(1) = ph(j);   % p(heads)\n",
    "    px(2) = 1-px(1); % p(tails))\n",
    "    for i=1:n\n",
    "        H(j) = H(j) + -( px(i) * log2(px(i)) );\n",
    "    end\n",
    "end\n",
    "figure(1), hold on\n",
    "plot(ph, H, 'LineWidth', 2)\n",
    "set(gca, 'FontSize', 20)\n",
    "xlabel('Probability of heads (coin bias)')\n",
    "ylabel('entropy (bits)')\n",
    "plot([.5 .5], get(gca, 'YLim'), 'k--')\n",
    "\n",
    "% entropy is maximized with maximum uncertainty and will max out at\n",
    "% log2(n)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%% PART III: Mutual information (MI). MI is a measure of how much knowing \n",
    "% about 1 variable tells you about the state of another variable. Putting\n",
    "% aside entropy and measures of uncertainty/variance for a minute, here is the \n",
    "% intuition. Suppose you have two variables that are completely unrelated\n",
    "% to each other: measuring one variable will tell you nothing about the\n",
    "% state of the other variable. In contrast, if you have two variables that\n",
    "% are perfectly correlated, then measureing one variable will tell you\n",
    "% everything about the state of the other. In this special (unusual) case,\n",
    "% the mutual information will be equal to the entropy of either variable\n",
    "% alone (that is: the information gained by measuring one variable will be\n",
    "% equal to the information gained by measuring both)\n",
    "%\n",
    "% to put this back in terms of entropy: lets say we have two variables, X\n",
    "% and Y. If we want to assess the MI between X and Y, then the intuition is\n",
    "% that we need to know the following difference score:\n",
    "%\n",
    "% (total entropy of X) -  (entropy of X given that we know Y). \n",
    "%\n",
    "% In other words, how much is uncertainty about X REDUCED when we measure Y?\n",
    "% That is the MI between the two variables. And it leads to the definition\n",
    "% of MI:\n",
    "%\n",
    "% MI = H(X) - H(X|Y)  where H(X) is the entropy of X, and H(X|Y) is the\n",
    "% conditional entropy of X given that we've measured Y - it the average\n",
    "% entropy of X across all values of Y\n",
    "% Going back to our\n",
    "% examples above - suppose X and Y are unrelated (independent):\n",
    "%\n",
    "% measuring Y doesn't tell you anything about X, so \n",
    "% H(X) == H(X|Y), so MI = H(X) - H(X) = 0\n",
    "%\n",
    "% On the other hand, if the two variables are perfectly related,\n",
    "% then knowing Y will not give us any reduction in uncertainty about X, so:\n",
    "% \n",
    "% H(X|Y) = 0, and MI = H(X) - 0, or MI = H(X)\n",
    "%\n",
    "%\n",
    "% Lets do a few examples - first generate two sequences that are randomly \n",
    "% generated idendently from each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discrete\n",
    "\n",
    "x = round(rand(1,1000));\n",
    "y = round(rand(1,1000));\n",
    "\n",
    "px(1) = sum(x)/numel(x);    % probability that x==1\n",
    "px(2) = 1-px(1);            % prob that x==0\n",
    "\n",
    "Hx = -sum( px .* log2(px) );    % do in one line instead of looping using the .* operator\n",
    "\n",
    "% then entropy of x given y (Hxy).\n",
    "% 1) For all Y {i=1:numel(Y)}, compute the entropy of X given each Y\n",
    "% 2) Multiply H(X|Yi) with the probability of each Y (i.e. pyi)\n",
    "% 3) Sum over all i\n",
    "% this is the average conditional entropy over all X's \n",
    "\n",
    "Hxy=0;\n",
    "uniquey = unique(y);\n",
    "uniquex = unique(x);\n",
    "for i=1:numel(uniquey) % loop over unique elements of y, in this case 0,1\n",
    "    \n",
    "    % probability that y==y(i) (prob of each y)\n",
    "    py = numel(find(y==uniquey(i)))/numel(y);\n",
    "\n",
    "    % then loop over all possible x's to compute entropy of x given each y\n",
    "    tmp=0;\n",
    "    for j=1:numel(uniquex)\n",
    "        px_y = numel(find(x==uniquex(j) & y==uniquey(i)))/numel(find(y==uniquey(i)));       % e.g. prob x==1 when y==0\n",
    "        tmp = tmp + (-( px_y .* log2(px_y) ));                                              % entropy      \n",
    "    end\n",
    "    \n",
    "    % then tally up entropy of x given specific y times probability of that\n",
    "    % y (py)\n",
    "    Hxy = Hxy + py*tmp;\n",
    "end\n",
    "\n",
    "% then we have everything we need to compute MI, which in this case should\n",
    "% be ~0 becuase the variables are completely independent!\n",
    "MI = Hx - Hxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function Hx = entropy(x)\n",
    "\n",
    "% js - 03032014\n",
    "% compute entropy of discrete variable x\n",
    "\n",
    "uniquex = unique(x);\n",
    "\n",
    "Hx = 0;\n",
    "for i=1:numel(uniquex)\n",
    "    px = numel(find(x==uniquex(i)))/numel(x);    % probability that x==uniquex(i)\n",
    "    if px~=0 % because log2(0) = -inf\n",
    "        Hx = Hx + (-sum( px * log2(px) ));    \n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function Hxy = condEntropy(x,y)\n",
    "% js, 03022014\n",
    "% conditional entropy of x|y\n",
    "% i.e. average entropy of x given each y\n",
    "% Hxy.\n",
    "% 1) For all Y {i=1:numel(X)}, compute the entropy of X given each Y\n",
    "% 2) Multiply H(X|Y==i) with the probability of each Y (i.e. pxi)\n",
    "% 3) Sum over all i\n",
    "\n",
    "Hxy=0;\n",
    "uniquey = unique(y);\n",
    "uniquex = unique(x);\n",
    "for i=1:numel(uniquey) % loop over unique elements of y\n",
    "    \n",
    "    % probability that y==y(i) (prob of each y)\n",
    "    py = numel(find(y==uniquey(i)))/numel(y);\n",
    "%     ny(i) = numel(find(y==uniquey(i)));\n",
    "    \n",
    "    % then loop over all possible x's to compute entropy of x given each y\n",
    "    tmp=0;\n",
    "    for j=1:numel(uniquex)\n",
    "        px_y = numel(find(x==uniquex(j) & y==uniquey(i)))/numel(find(y==uniquey(i)));     \n",
    "        if px_y~=0 \n",
    "            tmp = tmp + ( -sum( px_y .* log2(px_y) ));                                   % entropy      \n",
    "        end\n",
    "    end\n",
    "    \n",
    "    % then tally up entropy of x given specific y times probability of that\n",
    "    % y (py)\n",
    "    Hxy = Hxy + py*tmp;\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "function Hxy = mutualInfo(x,y)\n",
    "% js, 02272014\n",
    "\n",
    "% first compute the entropy of x\n",
    "px(1) = sum(x)/numel(x);   % probability that x==1\n",
    "px(2) = 1-px(1);           % prob that x==0\n",
    "\n",
    "% Then Hxy.\n",
    "% 1) For all X {i=1:numel(X)}, compute the entropy of Y given each X\n",
    "% 2) Multiply H(Y|X==i) with the probability of each X (i.e. pxi)\n",
    "% 3) Sum over all i\n",
    "\n",
    "Hxy=0;\n",
    "for i=1:numel(unique(x)) % loop over unique elements of x, in this case 0,1\n",
    "    \n",
    "    % then loop over all possible y's to compute entropy of y given each x\n",
    "    tmp=0;\n",
    "    for j=1:numel(unique(y))\n",
    "        py_x = numel(find(y==y(j) & x==x(i)))/numel(find(x==x(i)));     % e.g. prob y==1 when x==0\n",
    "        tmp = tmp + (-sum( py_x .* log2(py_x) ));                       % entropy      \n",
    "    end\n",
    "    \n",
    "    % then tally up entropy of y given specific x times px\n",
    "    Hxy = Hxy + px(i)*tmp;\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(1000)\n",
    "y = np.random.rand(1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 30\n",
    "def calc_MI(X,Y,bins = np.linspace(-90,90,n_bins+1)):\n",
    "    bad = np.isnan(X) | np.isnan(Y)\n",
    "    X = X[~bad]\n",
    "    Y = Y[~bad]\n",
    "    \n",
    "    c_XY = np.histogram2d(X,Y,bins)[0]\n",
    "    c_X = np.histogram(X,bins)[0]\n",
    "    c_Y = np.histogram(Y,bins)[0]\n",
    "\n",
    "    H_X = shan_entropy(c_X)\n",
    "    H_Y = shan_entropy(c_Y)\n",
    "    H_XY = shan_entropy(c_XY)\n",
    "    \n",
    "    MI = H_X + H_Y - H_XY\n",
    "    return MI\n",
    "\n",
    "def shan_entropy(c):\n",
    "    c_normalized = c / float(np.sum(c))\n",
    "    c_normalized = c_normalized[np.nonzero(c_normalized)]\n",
    "    H = -sum(c_normalized* np.log2(c_normalized))  \n",
    "    return H\n",
    "\n",
    "def calc_MI_ds(X,Y,n_samp=2000,n_iter=30,do_shuf=False,ret_all = False):\n",
    "    MIs = []\n",
    "    bad = np.isnan(X) | np.isnan(Y)\n",
    "    X = X[~bad]\n",
    "    Y = Y[~bad]\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        if do_shuf:\n",
    "            np.random.shuffle(X)\n",
    "        samp = np.random.choice(len(X),n_samp,replace=False)\n",
    "        MIs.append(calc_MI(X[samp],Y[samp]))\n",
    "    if ret_all:\n",
    "        return MIs\n",
    "    return (np.mean(MIs), np.std(MIs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.07719825221\n"
     ]
    }
   ],
   "source": [
    "x = np.random.rand(1000)*60\n",
    "y = np.random.rand(1000)*60\n",
    "n_bins = 600\n",
    "\n",
    "mi = calc_MI(X=x,Y=y,bins = np.linspace(-90,90,n_bins+1))\n",
    "print(mi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
