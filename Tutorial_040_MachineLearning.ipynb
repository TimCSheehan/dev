{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 04, Intro to machine learning/pattern classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning, or pattern recognition, refers to a large set of tools that are used in many areas of science in any case where you are trying to use multiple measured variables to determine if information is encoded about different levels of an experimental manipulation. \n",
    "* To the extent that there is information in a pattern of measured responses, then the pattern recognition algorithm will be able to successfully assign different examplars into their correct classes.\n",
    "* If there is no information, then it the pattern recognition algorithm will randomly guess and classifation accuracy will be at chance. \n",
    "* Note that the 'pattern' part of 'pattern recognition' refers to the fact that we're not just going to use a single variable to predict our outcome measure - we're going  to use the information encoded by a series of variables to make predictions (i.e. a 'multivariate' analysis).\n",
    "* The multivariate nature of the method is a major advantage as you might infer a null relationship based on univariate methods when in fact there is a very robust relationship that can be revealed by exploiting information encoded in a pattern of measurements. \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Cross-validation: To assess the generalizability of a pattern classification algorithm. Cross-validation simply refers to the notion that you train your pattern recognition algorithm (henceforth I'll call this a 'classifer') using one set of data, and then you validate, or 'test', the performance of the classifier using a novel set of data that was not part of the training set. The main purpose of cross-validation is to assess the generalizability of your classifier and its ability to correctly categorize novel inputs.\n",
    "\n",
    "This sounds simple but can be tricky...suppose you did an experiment that had 500 trials of stimulus type A and 500 trials of stimulus type B and you measured the response on each trial in 100 neurons. \n",
    "\n",
    "Then, to figure out how well the neural data respond systematically to changes in stimulus parameters, you  fit a multivariate regression analysis to see how much variability in the stimulus is accounted for by changes in neural activity (i.e. you compute something akin to a R^2 value to asses goodness of fit). Suppose you run this analysis on all 1,000 trials and you get your R^2 value and its nice and high - like .75 or so. You might be really happy with this, however, since you fit all of the data in your model, your estimate of how good the model fits the data is almost certainly overestimating how good the model is at accounting for the relationship between the two factors because your dependent variables  (your measured neural responses) are corrupted by noise, and this noise is idiosyncratic in the sense that if you were to perform the experiment again, you'd get 1000 different measurements that were similar to the first 1000, but corrupted by different noise. \n",
    "\n",
    "As a result, when you fit your model to the data, the resulting coeffecients will reflect the true 'signal' in the data AND the idiosyncratic noise that was measured along with the signal. In effect, your model learns the relationship between the independent variable and the (signal + noise). This occurs because your model has no a priori means of separating out signal and noise - it just gets a measure of neural responses that were evoked by each stimulus, and the model is just relating those measurements to the independent variables. This is referred to as 'overfitting', and is a exacerbated by small data sets (where the signal is not likely to emerge from the noise due to the small sample size) and when you have a model that has lots of free parameters (more free parameters means that the model can more flexibly account for random variations in the data...i.e. noise).  \n",
    "So - what to do? Instead of fitting the model to all the data and assessing the goodness of fit, you could use cross-validation to estimate **prediction accuracy**. In our example above, you could train your classifer using 400/500 of the trials associated with each stimulus set (so 800 trials total), and then 'test' the classifer's performance at guessing the correct stimulus class using the remaining 200 trials (100 associated with each stimulus). \n",
    "\n",
    "Then you could permute this train/test procedure several times, each time holding-out a different set of 800 trials to train the classifer and 200 trials to test the performance of the classifier. Here is the cool part: if your model is just learning the idiosyncratic noise in the data, then you might have a reasonable looking R^2 value based on your training data (i.e. the model fits the training data ok), but your ability to classify novel examplars from the test set will be at chance becuase your model just learned the random noise in the training set and there was no consistent 'signal' that could actually discriminate between conditions. \n",
    "\n",
    "So, the use of cross-validation can protect against overly optimistic assessments of model fit due to 'overfitting', and also enables you to assess the generalizability of the model to classify novel exemplars. The degree to which a classifer generalizes to correctly predict novel stimuli is really  then a measure of how much real signal - or information - there is in your data about the different examplars that you're trying to classify.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats  # has t-tests and other stats stuff...\n",
    "from scipy.linalg import eigh\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# also define the default font we'll use for figures. \n",
    "fig_font = {'fontname':'Arial', 'size':'20'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First lets set up some code to generate simulated data\n",
    "* We'll set up a function to generate data from two variables that is correlated to a specified degree\n",
    "* Can see why multivariate analyses are so powerful when comparing a N-D representation to a univariate representation\n",
    "* In this example, we'll have two variables (e.g. neurons), and two experimental conditions\n",
    "* We want to see if the pattern of responses across the two variables systematically varies across conditions\n",
    "* [scipy cookbook for generating correlated samples](https://scipy-cookbook.readthedocs.io/items/CorrelatedRandomSamples.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2924f6ed7b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# number of data points in our simulated data\n",
    "N = 100 \n",
    "\n",
    "# number of variables, in this case lets start with 2 because that makes it easy to \n",
    "# visualize\n",
    "V = 2\n",
    "\n",
    "# means of each variable in each condition\n",
    "mean_of_data0 = np.array([1,2])\n",
    "mean_of_data1 = np.array([4,1])\n",
    "\n",
    "# generate some random data vectors drawn from normal\n",
    "data0 = np.random.randn(N,V) \n",
    "data1 = np.random.randn(N,V) \n",
    "\n",
    "# set up a covariance matrix - main diag is the variance of each and off-diags are the cov\n",
    "# first do it for the responses in condition 1\n",
    "cv_mat0 = np.array([\n",
    "    [3.1, 3],\n",
    "    [2.3, 2]\n",
    "])\n",
    "\n",
    "# for our second data set...\n",
    "cv_mat1 = np.array([\n",
    "    [2.1, 2.5],\n",
    "    [2.1, 3.5]\n",
    "])\n",
    "\n",
    "# impose the covariance structure on the data\n",
    "\n",
    "# first compute the eigenvalues and eigenvectors\n",
    "evals, evecs = eigh(cv_mat)\n",
    "\n",
    "# Construct c, so c*c^T = cv_mat.\n",
    "c = np.dot(evecs, np.diag(np.sqrt(evals)))\n",
    "\n",
    "# convert the data using by multiplying data by c\n",
    "# to be consistent with previous tutorials, we want the data running down columns...so do the double .T\n",
    "cdata0 = np.dot(c, data0.T).T + np.vstack((np.ones(N,)*mean_of_data0[0], np.ones(N,)*mean_of_data0[1])).T\n",
    "\n",
    "cdata1 = np.dot(c, data1.T).T + np.vstack((np.ones(N,)*mean_of_data1[0], np.ones(N,)*mean_of_data1[1])).T  \n",
    "\n",
    "# plot the data...\n",
    "plt.scatter(cdata0[:,0], cdata0[:,1], color='b')\n",
    "plt.scatter(cdata1[:,0], cdata1[:,1], color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.00874786,  1.88406453])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(data0, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now lets add another data set..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% dprime = (mean1 - mean2)/(pooled estimate of noise across conditions)\n",
    "\n",
    "% so classAcc will be proportional to d-prime in this univariate\n",
    "% situation - moving the means farther apart (while holding noise constant)\n",
    "% will lead to a larger dprime and also to better classification, and reducing the\n",
    "% noise while holding the means constant will lead to responses that are\n",
    "% clustered closer to their means, so they will overlap less and classAcc\n",
    "% will be higher. This is an important link, because all of the\n",
    "% classification algorithms that we'll use today - including the\n",
    "% multivariate algorithms that are introduced below - are really just fancy\n",
    "% versions of dprime. \n",
    "\n",
    "%% REVIEW: We just did classification using a single variable based on a single\n",
    "% boundary point. The key notions to take away thus far are: (1) that the\n",
    "% separation of the observations from each condition will influence\n",
    "% classification accuracy, and (2) that we compute classification accuracy\n",
    "% by figuring out which side of a boundary each data point in the test set\n",
    "% falls on. This logic is all that you need to do pattern recognition in\n",
    "% more complex scenarios! Its really only how you determine the boundary that\n",
    "% changes as you add more variables. \n",
    "\n",
    "\n",
    "%% PART II: univariate vs multivarte classification\n",
    "% In this section, we'll examine the advantage of multivariate vs univariate\n",
    "% classification techniques. The pattern of measurements across many variables\n",
    "% (multivariate) can reveal effects that can be missed if each variable\n",
    "% is considered in isolation (univariate). To make this point, we'll set up\n",
    "% two variables and examine the observations in each variable in response\n",
    "% to condition A and condition B\n",
    "clear all\n",
    "close all\n",
    "\n",
    "% number of observations in our simulated study - same as above\n",
    "nObs = 200;\n",
    "nObsCond = nObs/2;     % store the number of observations per condition for convenience later on\n",
    "\n",
    "% NEW - number of variables that we're going to measure for conditions A\n",
    "% and B. For this section of the tutorial, always set to 2 so that we can\n",
    "% visualize more easily.\n",
    "nv = 2; % number of variables\n",
    "\n",
    "% response amplitude to conditions A and B for each of our variables\n",
    "ampA = [1,2];       % response of each variable in condition A  %%%%randn(1, nv)\n",
    "ampB = [2,1];       % response of each variable in condition B\n",
    "\n",
    "% magnitude of independent identically distributed noise (IID noise)\n",
    "n = .1;\n",
    "\n",
    "% generate a set of responses evoked by each stimulus\n",
    "% note the use of 'repmat' to replicate the ampA and ampB vectors so that\n",
    "% we can add them to a [nObsCond, nv] matrix of noise\n",
    "rA = repmat(ampA, nObsCond, 1)  + (n*randn(nObsCond, nv));    % set up a matrix that is nObsCond x num variables for cond A\n",
    "rB = repmat(ampB, nObsCond, 1) + (n*randn(nObsCond, nv));     % same kind of matrix for cond B\n",
    "\n",
    "% plot the data in variable 1 against the data in variable 2\n",
    "figure(1), clf, hold on\n",
    "plot(rA(:,1), rA(:,2), 'ro', 'MarkerSize', 10)\n",
    "plot(rB(:,1), rB(:,2), 'bv', 'MarkerSize', 10)\n",
    "set(gca, 'FontSize', 20)     \n",
    "minData = min([rA(:);rB(:)]);           % for adjusting axes\n",
    "maxData = max([rA(:);rB(:)]);\n",
    "set(gca, 'XLim', [minData, maxData])    % plot on a scale that spans the range of the data\n",
    "set(gca, 'YLim', [minData, maxData])    % plot on a scale that spans the range of the data\n",
    "xlabel('Response variable 1')\n",
    "ylabel('Response variable 2')\n",
    "legend({'Cond A', 'Cond B'})\n",
    "\n",
    "% now, instead of a point, we want to draw a line that best\n",
    "% separates these classes. For now, we'll just draw a line that goes\n",
    "% between the mean responses of each variable to condition A and condition\n",
    "% B - in practice, figuring out how to draw this line depends on the\n",
    "% pattern recognition algorithm that you actually use.\n",
    "upperLineX = max([rA(:,1);rB(:,1)]);    % max x-value (max response in voxel 1 across conditions)\n",
    "upperLineY = max([rA(:,2);rB(:,2)]);    % max y-value (max response in voxel 2 across conditions)\n",
    "lowerLineX = min([rA(:,1);rB(:,1)]);    % min x\n",
    "lowerLineY = min([rA(:,2);rB(:,2)]);    % min y\n",
    "plot([lowerLineX, upperLineX], [lowerLineY, upperLineY], 'k', 'LineWidth', 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% classify based on the response to each condition, collapsed across our\n",
    "% two variables. To save space I've written a function to compute univariate\n",
    "% classification accuracy - it does exactly what the 'for\n",
    "% i=1:numPerms...end' loop does above. \n",
    "\n",
    "holdOut = nObsCond;         % as above, set to 1/2 of the data.\n",
    "nPerms = 100;               % number of cross-validation 'folds' or permutations that we'll carry out to estimate classification accuracy\n",
    "labels = [ones(nObsCond,1); ones(nObsCond,1)+1];  % this is a vector that tells us which condition a given observation belongs to - we'll use this to evaluate classification accuracy\n",
    "data = [rA; rB];            % vertically concatenate the observations from each condition into a single data vector\n",
    "\n",
    "% here is the function call that does what we wrote out in the tutorial\n",
    "% above already. It should be right around chance (50%) classification\n",
    "% accuracy, because we're discarding the pattern of responses across\n",
    "% voxels. Furthermore, reducing the noise won't help either, because the\n",
    "% expected mean response to each condition, averaged across voxels, is identical \n",
    "classAcc = uniClass(data, labels, holdOut, nPerms)\n",
    "\n",
    "% Now lets actually consider the pattern of responses across voxels to see\n",
    "% how well we can classify the data. Just by looking at the plot, it should\n",
    "% be apparent that this should work pretty well. To start, we'll use a\n",
    "% simple classifier based just on correlation coeffecients. We'll just\n",
    "% correlate each vector in the test data set (a vector with just two values\n",
    "% in it in this example) with the mean pattern across variables assoicated\n",
    "% with condition A and the mean pattern across variables associated with\n",
    "% condition B. To do this, we'll use another function that is identical to\n",
    "% the uniClass function above, but this time it will use correlation\n",
    "% coeffecients instead of univariate means.\n",
    "classAcc = corrClass(data, labels, holdOut, nPerms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% an example of how correlated variables can make a big difference.\n",
    "\n",
    "clear;\n",
    "close all;\n",
    "rng(0);\n",
    "% number of observations in our simulated study - same as above\n",
    "nObs = 1000;\n",
    "nObsCond = nObs/2;     % store the number of observations per condition for convenience later on\n",
    "\n",
    "% set up correlated responses. Sigma is the covariance matrix, where the\n",
    "% diagnoal elements correspond to variance of each response, and the off\n",
    "% diagonal elements correspond to the covariance betweeen variables\n",
    "\n",
    "% generate responses in two voxels to condition A\n",
    "ampA = [2 4]; Sigma = [2 1.5; 1.5 2];\n",
    "rA = mvnrnd(ampA, Sigma, nObsCond);\n",
    "\n",
    "% responses in two voxels for condition B\n",
    "ampB = [2 1]; Sigma = [2 1.5; 1.5 2];\n",
    "rB = mvnrnd(ampB, Sigma, nObsCond);\n",
    "\n",
    "% plot the data in variable 1 against the data in variable 2\n",
    "figure(1), clf, hold on\n",
    "plot(rA(:,1), rA(:,2), 'ro', 'MarkerSize', 10)\n",
    "plot(rB(:,1), rB(:,2), 'bv', 'MarkerSize', 10)\n",
    "set(gca, 'FontSize', 20)     \n",
    "minData = min([rA(:);rB(:)]);           % for adjusting axes\n",
    "maxData = max([rA(:);rB(:)]);\n",
    "set(gca, 'XLim', [minData, maxData])    % plot on a scale that spans the range of the data\n",
    "set(gca, 'YLim', [minData, maxData])    % plot on a scale that spans the range of the data\n",
    "xlabel('Response variable 1')\n",
    "ylabel('Response variable 2')\n",
    "legend({'Cond A', 'Cond B'})\n",
    "\n",
    "data = [rA; rB];\n",
    "labels = [ones(size(rA,1),1); ones(size(rB,1),1)+1];\n",
    "\n",
    "classAcc = normEucClass(data, labels, 100, 100)\n",
    "classAcc = mahaClass(data, labels, 100, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%% quick demo of what incorporating the covariance does - \"Maha whitening\"\n",
    "clear\n",
    "close all\n",
    "% quick demo of what accounting for cov does for maha distance based\n",
    "% calculations\n",
    "mu = [2,3];\n",
    "sigma = [1,1.5;1.5,3];\n",
    "data = mvnrnd(mu,sigma,100);\n",
    "\n",
    "figure(1)\n",
    "scatter(data(:,1), data(:,2));\n",
    "xlabel('response in var 1')\n",
    "ylabel('response in var 2')\n",
    "set(gca, 'FontSize', 24)\n",
    "\n",
    "% estimate cov matrix\n",
    "cm = cov(data);\n",
    "\n",
    "% whiten data by multiplying by inverse of covariance matrix\n",
    "data_cm = data * inv(sqrtm(cm));\n",
    "               \n",
    "% replot...notice that the correlations are gone!\n",
    "figure(2)\n",
    "scatter(data_cm(:,1), data_cm(:,2));\n",
    "xlabel('response in var 1')\n",
    "ylabel('response in var 2')\n",
    "set(gca, 'FontSize', 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%% significance testing...permute labels...\n",
    "clear\n",
    "close all\n",
    "\n",
    "% number of observations in our simulated study - same as above\n",
    "nObs = 200;\n",
    "nObsCond = nObs/2;     % store the number of observations per condition for convenience later on\n",
    "\n",
    "nv = 2; % number of variables\n",
    "\n",
    "% response amplitude to conditions A and B for each of our variables\n",
    "ampA = [1,1.5];       % response of each variable in condition A  %%%%randn(1, nv)\n",
    "ampB = [1.5,1];       % response of each variable in condition B\n",
    "\n",
    "% magnitude of independent identically distributed noise (IID noise)\n",
    "n1 = 2.5;\n",
    "n2 = 2.5;\n",
    "\n",
    "% generate a set of responses evoked by each stimulus\n",
    "rA = repmat(ampA, nObsCond, 1)  + (n1*randn(nObsCond, nv));    % set up a matrix that is nObsCond x num variables for cond A\n",
    "rB = repmat(ampB, nObsCond, 1) + (n2*randn(nObsCond, nv));     % same kind of matrix for cond B\n",
    "\n",
    "holdOut = nObsCond;         % as above, set to 1/2 of the data.\n",
    "nPerms = 100;               % number of cross-validation 'folds' or permutations that we'll carry out to estimate classification accuracy\n",
    "labels = [ones(nObsCond,1); ones(nObsCond,1)+1];  % this is a vector that tells us which condition a given observation belongs to - we'll use this to evaluate classification accuracy\n",
    "data = [rA; rB];   \n",
    "\n",
    "% do the classification\n",
    "classAcc = mahaClass(data, labels, 100, 100)\n",
    "\n",
    "for j = 1:100\n",
    "    newLabel = labels(randperm(numel(labels)));\n",
    "    newAcc(j) = mahaClass(data, newLabel, 100, 100);\n",
    "end\n",
    "\n",
    "p = 1-numel(find(classAcc>newAcc))/numel(newAcc)\n",
    "\n",
    "p = 1-numel(find(.51>newAcc))/numel(newAcc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
